{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Открываем тренировочную и тестовую выборку ПЕРВОГО датасета**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1train = pd.read_csv('dataset1_train_11414r.csv',encoding='utf-8')\n",
    "DS1train_without_stopwords = pd.DataFrame(columns=['comment', 'toxic'])\n",
    "\n",
    "DS1test = pd.read_csv('dataset1_test_2999r.csv',encoding='utf-8')\n",
    "DS1test_without_stopwords = pd.DataFrame(columns=['comment', 'toxic'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Открываем тренировочную и тестовую выборку ВТОРОГО датасета**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS2train = pd.read_csv('dataset2_train_126834r.csv',encoding='utf-8')\n",
    "DS2train_without_stopwords = pd.DataFrame(columns=['comment', 'toxic'])\n",
    "\n",
    "DS2test = pd.read_csv('dataset2_test_100000r.csv',encoding='utf-8')\n",
    "DS2test_without_stopwords = pd.DataFrame(columns=['comment', 'toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция для чистки датасетов от некоторых пунктуационных символов и стоп-слов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_me(comment):\n",
    "    tokens =nltk.word_tokenize(comment.lower())\n",
    "    tokens = [i for i in tokens if ( i not in string.punctuation or i in ['!','?','не','-',')','('] ) ]\n",
    "    \n",
    "    stop_words = stopwords.words('russian')\n",
    "    stop_words.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', 'к', 'на'])\n",
    "    \n",
    "    \n",
    "    tokens = [i for i in tokens if ( i not in stop_words)]\n",
    "    tokens = [i.replace(\"«\", \"\").replace(\"»\", \"\") for i in tokens]\n",
    "\n",
    "    st=''\n",
    "    for k in range(len(tokens)):\n",
    "        st+=tokens[k]+' '\n",
    "    \n",
    "    return st.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проводим чистку тренировочной и тестовой выборки из ПЕРВОГО датасета от некоторых пунктуационных символов и стоп-слов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(DS1train.index.size):\n",
    "    clean_str=tokenize_me(DS1train['comment'][i])\n",
    "    DS1train_without_stopwords=DS1train_without_stopwords.append({'comment':clean_str,'toxic': str(DS1train['toxic'][i])},ignore_index=True)\n",
    "\n",
    "for j in range(DS1test.index.size):\n",
    "    clean_str=tokenize_me(DS1test['comment'][j])\n",
    "    DS1test_without_stopwords=DS1test_without_stopwords.append({'comment': clean_str,'toxic':str(DS1test['toxic'][j])},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проводим чистку тренировочной и тестовой выборки из ВТОРОГО датасета от некоторых пунктуационных символов и стоп-слов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(DS2train.index.size):\n",
    "    clean_str=tokenize_me(DS2train['comment'][i])\n",
    "    DS2train_without_stopwords=DS2train_without_stopwords.append({'comment':clean_str,'toxic': str(DS2train['toxic'][i])},ignore_index=True)\n",
    "\n",
    "for j in range(DS1test.index.size):\n",
    "    clean_str=tokenize_me(DS2test['comment'][j])\n",
    "    DS2test_without_stopwords=DS2test_without_stopwords.append({'comment': clean_str,'toxic':str(DS2test['toxic'][j])},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохроняем очищенные данные тренировочной и тестовой выборки из ПЕРВОГО датасета**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('dataset1_train_11414r_without_sw_pnct.csv','w',encoding='utf8') as DS1train_inf:\n",
    "    DS1train_inf.write(\"comment,toxic\"+'\\n')\n",
    "    for i in range(DS1train.index.size):\n",
    "        if not ( DS1train_without_stopwords['comment'][i] == '' ):\n",
    "            DS1train_inf.write('\\\"'+DS1train_without_stopwords['comment'][i]+'\\\"'+','+'\\\"'+DS1train_without_stopwords['toxic'][i]+'\\\"'+'\\n')\n",
    "            \n",
    "with open('dataset1_test_2999r_without_sw_pnct.csv','w',encoding='utf8') as DS1test_inf:\n",
    "    DS1test_inf.write(\"comment,toxic\"+'\\n')\n",
    "    for j in range(DS1test.index.size):\n",
    "        if not ( DS1test_without_stopwords['comment'][j] == '' ):\n",
    "            DS1test_inf.write('\\\"'+DS1test_without_stopwords['comment'][j]+'\\\"'+','+'\\\"'+DS1test_without_stopwords['toxic'][j]+'\\\"'+'\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохроняем очищенные данные тренировочной и тестовой выборки из ВТОРОГО датасета**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset2_train_126834r_without_sw_pnct.csv','w',encoding='utf8') as DS2train_inf:\n",
    "    DS2train_inf.write(\"comment,toxic\"+'\\n')\n",
    "    for i in range(DS2train.index.size):\n",
    "        if not ( DS2train_without_stopwords['comment'][i] == '' ):\n",
    "            DS2train_inf.write('\\\"'+DS2train_without_stopwords['comment'][i]+'\\\"'+','+'\\\"'+DS2train_without_stopwords['toxic'][i]+'\\\"'+'\\n')\n",
    "            \n",
    "with open('dataset2_test_100000r_without_sw_pnct.csv','w',encoding='utf8') as DS2test_inf:\n",
    "    DS2test_inf.write(\"comment,toxic\"+'\\n')\n",
    "    for j in range(DS2test.index.size):\n",
    "        if not ( DS2test_without_stopwords['comment'][j] == '' ):\n",
    "            DS2test_inf.write('\\\"'+DS2test_without_stopwords['comment'][j]+'\\\"'+','+'\\\"'+DS2test_without_stopwords['toxic'][j]+'\\\"'+'\\n')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
